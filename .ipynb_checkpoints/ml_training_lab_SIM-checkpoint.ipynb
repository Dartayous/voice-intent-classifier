{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c563e0-8d41-4da3-90d9-727bde1c4dbb",
   "metadata": {},
   "source": [
    "## Train a real ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e86c48-edd2-427a-b6f1-48f1980a6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47040a07-10da-4db1-b4ca-25331891c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"F:/ML_DATASETS/NLP/Voice Search AI Conversational Queries 2025/voice_search_query_captures.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab446a8c-1f34-4758-88e6-3fa501bca7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id                               user_id            timestamp  \\\n",
      "0         1  bdd640fb-0667-4ad1-9c80-317fa3b1799d  2025-04-17 19:27:32   \n",
      "1         2  bc8960a9-23b8-41e9-b924-56de3eb13b90  2025-02-09 19:19:27   \n",
      "2         3  8b9d2434-e465-4150-bd9c-66b3ad3c2d6d  2025-02-03 18:19:43   \n",
      "3         4  07a0ca6e-0822-48f3-ac03-1199972a8469  2025-02-06 09:18:10   \n",
      "4         5  9a1de644-815e-46d1-bb8f-aa1837f8a88b  2025-01-11 07:19:59   \n",
      "\n",
      "     device_type                      query_text language       intent  \\\n",
      "0     smartphone   How tall is the Eiffel Tower?  Spanish  information   \n",
      "1     smartphone           Track my Amazon order  English     shopping   \n",
      "2  car assistant  Define artificial intelligence  Spanish  information   \n",
      "3  car assistant           Set an alarm for 7 AM  Spanish      command   \n",
      "4  smart speaker           Set an alarm for 7 AM  English      command   \n",
      "\n",
      "      location  query_duration_sec  num_words is_successful  confidence_score  \\\n",
      "0  Los Angeles                5.99        6.0         False              0.87   \n",
      "1     New York                4.72        4.0          True              0.61   \n",
      "2        Paris                6.30        3.0          True              0.71   \n",
      "3        Paris                8.05        6.0          True              0.67   \n",
      "4        Delhi                4.53        6.0          True              0.68   \n",
      "\n",
      "  device_os_version  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2        Android 10  \n",
      "3            iOS 15  \n",
      "4        Android 11  \n",
      "Index(['query_id', 'user_id', 'timestamp', 'device_type', 'query_text',\n",
      "       'language', 'intent', 'location', 'query_duration_sec', 'num_words',\n",
      "       'is_successful', 'confidence_score', 'device_os_version'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Peek at the data\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe034-29a0-46a8-939e-519562e1e901",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 2a: Clean and Tokenize the Text\n",
    "I'm starting with the query_text column:\n",
    "\n",
    "* Lowercase everything\n",
    "\n",
    "* Remove punctuation\n",
    "\n",
    "* Tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94944237-7448-4b0a-a17d-c1fa0cfec802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    return text\n",
    "\n",
    "df['clean_query'] = df['query_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92d4a25e-da31-42eb-a531-db711d0c38a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       query_text                     clean_query\n",
      "0   How tall is the Eiffel Tower?    how tall is the eiffel tower\n",
      "1           Track my Amazon order           track my amazon order\n",
      "2  Define artificial intelligence  define artificial intelligence\n",
      "3           Set an alarm for 7 AM           set an alarm for 7 am\n",
      "4           Set an alarm for 7 AM           set an alarm for 7 am\n"
     ]
    }
   ],
   "source": [
    "# Preview cleaned text\n",
    "print(df[['query_text', 'clean_query']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb6dc2-9516-4f34-aaee-f90c8395cb6d",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 2b: Encode the Text\n",
    "Converting words into numbers using tokenization + padding. Using Tokenizer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbb097c7-eb91-4680-a8b7-e004fd2aceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['clean_query'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_query'])\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bdf9acb-2bdf-4fe8-8fa5-5e23a16ffd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 54 14  3 55 56  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [57 11 58 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 82 83  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [45 46 47 48 49 50  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [45 46 47 48 49 50  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d260c9-590c-4caa-97f6-5c5e5d061a40",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 2c: Encode the Labels (intent)\n",
    "Turning the intent column into numerical targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a485b4cb-7b39-4667-9ba0-3af2fc55caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['intent_encoded'] = label_encoder.fit_transform(df['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29cb0e4a-ee05-4a86-a795-bbc384e9bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          intent  intent_encoded\n",
      "0    information               2\n",
      "1       shopping               4\n",
      "3        command               0\n",
      "7     navigation               3\n",
      "8  entertainment               1\n"
     ]
    }
   ],
   "source": [
    "print(df[['intent', 'intent_encoded']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb5dd4-5270-415b-b242-277be9d0da62",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 3: Train/Test Split\n",
    "Now splitting the data so I can train and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dc1754d-6599-4c49-9ead-c504a688c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sequences\n",
    "y = df['intent_encoded'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962a8cd-1ea9-4590-92a5-5add77dfc42c",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 4a: Define the Model Architecture and Compile\n",
    "I'll use TensorFlow/Keras to build a simple feedforward neural network. Itâ€™ll take my padded query sequences as input and predict the intent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c65701a2-3e08-4687-8129-ee174ff791b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_1           â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_1           â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = 1000  # same as tokenizer num_words\n",
    "embedding_dim = 16\n",
    "max_length = 20    # same as pad_sequences maxlen\n",
    "num_classes = len(df['intent'].unique())\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9726b-c17f-4042-be07-3c374f3f15fd",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 4b: Train the Model\n",
    "Now feeding in my training data:\n",
    "\n",
    "This will:\n",
    "* Train for 10 epochs\n",
    "\n",
    "* Show accuracy and loss for both train and test sets\n",
    "\n",
    "* Give you a history object for plotting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d62b743-e266-44b9-a680-6803a3bb9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39/39 - 1s - 31ms/step - accuracy: 0.2178 - loss: 1.6004 - val_accuracy: 0.2283 - val_loss: 1.5840\n",
      "Epoch 2/10\n",
      "39/39 - 0s - 5ms/step - accuracy: 0.3248 - loss: 1.5613 - val_accuracy: 0.3248 - val_loss: 1.5288\n",
      "Epoch 3/10\n",
      "39/39 - 0s - 3ms/step - accuracy: 0.5691 - loss: 1.4720 - val_accuracy: 0.7653 - val_loss: 1.3940\n",
      "Epoch 4/10\n",
      "39/39 - 0s - 3ms/step - accuracy: 0.7605 - loss: 1.3048 - val_accuracy: 0.8714 - val_loss: 1.1889\n",
      "Epoch 5/10\n",
      "39/39 - 0s - 4ms/step - accuracy: 0.8569 - loss: 1.0763 - val_accuracy: 0.9260 - val_loss: 0.9386\n",
      "Epoch 6/10\n",
      "39/39 - 0s - 4ms/step - accuracy: 0.9735 - loss: 0.8246 - val_accuracy: 1.0000 - val_loss: 0.6956\n",
      "Epoch 7/10\n",
      "39/39 - 0s - 4ms/step - accuracy: 0.9839 - loss: 0.5989 - val_accuracy: 1.0000 - val_loss: 0.4966\n",
      "Epoch 8/10\n",
      "39/39 - 0s - 4ms/step - accuracy: 1.0000 - loss: 0.4216 - val_accuracy: 1.0000 - val_loss: 0.3500\n",
      "Epoch 9/10\n",
      "39/39 - 0s - 4ms/step - accuracy: 1.0000 - loss: 0.2951 - val_accuracy: 1.0000 - val_loss: 0.2440\n",
      "Epoch 10/10\n",
      "39/39 - 0s - 3ms/step - accuracy: 1.0000 - loss: 0.2079 - val_accuracy: 1.0000 - val_loss: 0.1740\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef4632-f6e2-45d3-8cd0-f4389e774664",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 5a: Prepare Unlabeled Queries\n",
    "\n",
    "Letâ€™s simulate a few new voice search inputs. You can either:\n",
    "\n",
    "* Pull real examples from your dataset (without the intent)\n",
    "\n",
    "* Or manually define a few test queries like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78650149-5802-4aea-b764-822763d306dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = [\n",
    "    \"Play jazz music on Spotify\",\n",
    "    \"What's the weather in Tokyo tomorrow?\",\n",
    "    \"Turn off the living room lights\",\n",
    "    \"Order me a pizza from Domino's\",\n",
    "    \"How far is the moon from Earth?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7724889-6f36-474f-ad65-34c7fd9d4d0c",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 5b: Preprocess Like Training\n",
    "We need to clean and tokenize these new queries using the same pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75067236-1bfe-43ae-a2c2-383e9d9bd354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "cleaned_queries = [clean_text(q) for q in new_queries]\n",
    "\n",
    "# Tokenize and pad\n",
    "new_sequences = tokenizer.texts_to_sequences(cleaned_queries)\n",
    "new_padded = pad_sequences(new_sequences, padding='post', maxlen=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c8ed3-dc53-4c4b-88a2-aec6f5155925",
   "metadata": {},
   "source": [
    "## ğŸ¬ Step 5c: Make Predictions (TEST on unlabeled data)\n",
    "Now we feed the preprocessed queries into your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d4c479e-3f61-4a60-bb52-a505d7235e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Query: 'Play jazz music on Spotify' â†’ Predicted Intent: 'entertainment'\n",
      "Query: 'What's the weather in Tokyo tomorrow?' â†’ Predicted Intent: 'information'\n",
      "Query: 'Turn off the living room lights' â†’ Predicted Intent: 'command'\n",
      "Query: 'Order me a pizza from Domino's' â†’ Predicted Intent: 'shopping'\n",
      "Query: 'How far is the moon from Earth?' â†’ Predicted Intent: 'information'\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(new_padded)\n",
    "predicted_labels = [label_encoder.inverse_transform([np.argmax(p)])[0] for p in predictions]\n",
    "\n",
    "for query, label in zip(new_queries, predicted_labels):\n",
    "    print(f\"Query: '{query}' â†’ Predicted Intent: '{label}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu-lab)",
   "language": "python",
   "name": "gpu-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
